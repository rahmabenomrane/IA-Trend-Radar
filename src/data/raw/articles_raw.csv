id,title,url,source,published_at,raw_text
1,pg-sui 1.6.11,https://pypi.org/project/pg-sui/1.6.11/,Pypi.org,2025-11-21T18:37:32Z,"PG-SUI

Population Genomic Supervised and Unsupervised Imputation.

About PG-SUI

PG-SUI is a Python 3 API that uses machine learning to impute missing values from population genomic SNP data. There are several supervised and unsupervised machine learning algorithms available to impute missing data, as well as some non-machine learning imputers that are useful.

Below is some general information and a basic tutorial. For more detailed information, see our API Documentation.

Supervised Imputation Methods

Supervised methods utilze the scikit-learn's IterativeImputer, which is based on the MICE (Multivariate Imputation by Chained Equations) algorithm (1), and iterates over each SNP site (i.e., feature) while uses the N nearest neighbor features to inform the imputation. The number of nearest features can be adjusted by users. IterativeImputer currently works with any of the following scikit-learn classifiers:

K-Nearest Neighbors

Random Forest

XGBoost

See the scikit-learn documentation (https://scikit-learn.org) for more information on IterativeImputer and each of the classifiers.

Unsupervised Imputation Methods

Unsupervised imputers include three custom neural network models:

Variational Autoencoder (VAE) (2)

Standard Autoencoder (SAE) (3)

Non-linear Principal Component Analysis (NLPCA) (4)

Unsupervised Backpropagation (UBP) (5)

VAE models train themselves to reconstruct their input (i.e., the genotypes). To use VAE for imputation, the missing values are masked and the VAE model gets trained to reconstruct only on known values. Once the model is trained, it is then used to predict the missing values.

SAE is a standard autoencoder that trains the input to predict itself. As with VAE, missing values are masked and the model gets trained only on known values. Predictions are then made on the missing values.

NLPCA initializes random, reduced-dimensional input, then trains itself by using the known values (i.e., genotypes) as targets and refining the random input until it accurately predicts the genotype output. The trained model can then predict the missing values.

UBP is an extension of NLPCA that runs over three phases. Phase 1 refines the randomly generated, reduced-dimensional input in a single layer perceptron neural network to obtain good initial input values. Phase 2 uses the refined reduced-dimensional input from phase 1 as input into a multi-layer perceptron (MLP), but in Phase 2 only the neural network weights are refined. Phase three uses an MLP to refine both the weights and the reduced-dimensional input. Once the model is trained, it can be used to predict the missing values.

Non-Machine Learning Methods

We also include several non-machine learning options for imputing missing data, including:

Per-population mode per SNP site

Global mode per SNP site

Using a phylogeny as input to inform the imputation

Matrix Factorization

These four ""simple"" imputation methods can be used as standalone imputers, as the initial imputation strategy for IterativeImputer (at least one method is required to be chosen), and to validate the accuracy of both IterativeImputer and the neural network models.

Installing PG-SUI

The easiest way to install PG-SUI is to use pip:

pip install pg-sui

If you have an Intel CPU and want to use the sklearn-genetic-intelex package to speed up scikit-learn computations, you can do:

pip install pg-sui[intel]

Manual Installation

Dependencies

python >= 3.11

pandas

numpy

scipy

matplotlib

seaborn

plotly

kaleido

tqdm

toytree

scikit-learn

xgboost

snpio

optuna

Installation troubleshooting

""use_2to3 is invalid"" error

Users running setuptools v58 may encounter this error during the last step of installation, using pip to install sklearn-genetic-opt:

ERROR: Command errored out with exit status 1: command: /Users/tyler/miniforge3/envs/pg-sui/bin/python3.8 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/6x/t6g4kn711z5cxmc2_tvq0mlw0000gn/T/pip-install-6y5g_mhs/deap_1d32f65d60a44056bd7031f3aad44571/setup.py'""'""'; __file__='""'""'/private/var/folders/6x/t6g4kn711z5cxmc2_tvq0mlw0000gn/T/pip-install-6y5g_mhs/deap_1d32f65d60a44056bd7031f3aad44571/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r

'""'""', '""'""'

'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /private/var/folders/6x/t6g4kn711z5cxmc2_tvq0mlw0000gn/T/pip-pip-egg-info-7hg3hcq2 cwd: /private/var/folders/6x/t6g4kn711z5cxmc2_tvq0mlw0000gn/T/pip-install-6y5g_mhs/deap_1d32f65d60a44056bd7031f3aad44571/ Complete output (1 lines): error in deap setup command: use_2to3 is invalid.

This occurs during the installation of DEAP, one of the dependencies for sklearn-genetic-opt. As a workaround, first downgrade setuptools, and then proceed with the installation as normal:

pip install setuptools==57 pip install sklearn-genetic-opt[all]

Mac ARM architecture

PG-SUI has been tested on the new Mac M1 chips and is working fine, but some changes to the installation process were necessary as of 9-December-21. Installation was successful using the following:

### Install Miniforge3 instead of Miniconda3 ### Download: https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh bash ~/Downloads/Miniforge3-MacOSX-arm64.sh # Close and re-open terminal # #create and activate conda environment conda create -n pg-sui python #activate environment conda activate pg-sui #install packages conda install -c conda-forge matplotlib seaborn jupyterlab scikit-learn tqdm pandas numpy scipy xgboost lightgbm tensorflow keras sklearn-genetic-opt toytree conda install -c bioconda pyvolve #downgrade setuptools (may or may not be necessary) pip install setuptools==57 #install sklearn-genetic-opt and mlflow pip install sklearn-genetic-opt mlflow

Any other problems we run into testing on the Mac ARM architecture will be adjusted here. Note that the step installing scikit-learn-intelex was skipped here. PG-SUI will automatically detect the CPU architecture you are running, and forgo importing this package (which will only work on Intel processors)

Input Data

You can read your input files as a GenotypeData object from the SNPio package:

# Import snpio. Automatically installed with pgsui when using pip. from snpio import GenotypeData # Read in PHYLIP, VCF, or STRUCTURE-formatted alignments. data = GenotypeData( filename=""example_data/phylip_files/phylogen_nomx.u.snps.phy"", popmapfile=""example_data/popmaps/phylogen_nomx.popmap"", force_popmap=True, filetype=""auto"", qmatrix_iqtree=""example_data/trees/test.qmat"", siterates_iqtree=""example_data/trees/test.rate"", guidetree=""example_data/trees/test.tre"", include_pops=[""EA"", ""TT"", ""GU""], # Only include these populations. There's also an exclude_pops option that will exclude the provided populations. )

Supported Imputation Methods

There are numerous supported algorithms to impute missing data. Each one can be run by calling the corresponding class. You must provide a GenotypeData instance as the first positional argument.

You can import all the supported methods with:

from pgsui import *

Or you can import them one at a time.

from pgsui import ImputeVAE

Supervised Imputers

Various supervised imputation options are supported:

# Supervised IterativeImputer classifiers knn = ImputeKNN(data) # K-Nearest Neighbors rf = ImputeRandomForest(data) # Random Forest or Extra Trees xgb = ImputeXGBoost(data) # XGBoost

Non-machine learning methods

Use phylogeny to inform imputation:

phylo = ImputePhylo(data)

Use by-population or global allele frequency to inform imputation

pop_af = ImputeAlleleFreq(data, by_populations=True) global_af = ImputeAlleleFreq(data, by_populations=False) ref_af = ImputeRefAllele(data)

Non-matrix factorization:

mf = ImputeMF(*args) # Matrix factorization

Unsupervised Neural Networks

vae = ImputeVAE ( data ) # Variational autoencoder nlpca = ImputeNLPCA ( data ) # Nonlinear PCA ubp = ImputeUBP ( data ) # Unsupervised backpropagation sae = ImputeStandardAutoEncoder ( data ) # standard autoencoder

Command-Line Interface

Run the PG-SUI CLI with pg-sui (installed alongside the library). The CLI follows the same precedence model as the Python API:

code defaults < preset (--preset) < YAML (--config) < explicit CLI flags < --set key=value .

Recent releases add explicit switches for the simulated-missingness workflow shared by the neural and supervised models:

--sim-strategy selects one of random , random_weighted , random_weighted_inv , nonrandom , nonrandom_weighted .

selects one of , , , , . --sim-prop sets the proportion of observed calls to temporarily mask when building the evaluation set.

sets the proportion of observed calls to temporarily mask when building the evaluation set. --simulate-missing disables simulated masking entirely (store-false flag); omit it to inherit preset/YAML defaults or re-enable via --set sim.simulate_missing=True .

Example:

pg-sui \ --vcf data.vcf.gz \ --popmap pops.popmap \ --models ImputeUBP ImputeVAE \ --preset balanced \ --sim-strategy random_weighted_inv \ --sim-prop 0.25 \ --set io.prefix=vae_vs_ubp

CLI overrides cascade into every selected model, so a single invocation can evaluate multiple imputers with a consistent simulation strategy and output prefix.

To-Dos

simulations

Documentation

References:

1. Stef van Buuren, Karin Groothuis-Oudshoorn (2011). mice: Multivariate Imputation by Chained Equations in R. Journal of Statistical Software 45: 1-67.

2. Kingma, D.P. & Welling, M. (2013). Auto-encoding variational bayes. In: Proceedings of the International Conference on Learning Representations (ICLR). arXiv:1312.6114 [stat.ML].

3. Hinton, G.E., & Salakhutdinov, R.R. (2006). Reducing the dimensionality of data with neural networks. Science, 313(5786), 504-507.

4. Scholz, M., Kaplan, F., Guy, C. L., Kopka, J., & Selbig, J. (2005). Non-linear PCA: a missing data approach. Bioinformatics, 21(20), 3887-3895.

5. Gashler, M. S., Smith, M. R., Morris, R., & Martinez, T. (2016). Missing value imputation with unsupervised backpropagation. Computational Intelligence, 32(2), 196-215."
2,Dell Ship Pro Max 16 Plus Laptops with Qualcomm's Discrete NPU,https://www.techpowerup.com/343143/dell-ship-pro-max-16-plus-laptops-with-qualcomms-discrete-npu,Techpowerup.com,2025-11-21T17:17:25Z,"If you're in the market for a new laptop and notice that the Dell Pro Max 16 Plus boasts more TOPS than usual, it's because Dell is equipping its latest models with a discrete Neural Processing Unit (NPU) from Qualcomm. The Dell Pro Max 16 Plus features a Qualcomm AI-100 Ultra NPU, delivering 870 INT8 TOPS within a 150 W TDP envelope, along with 128 GB of LPDDR5X memory. This provides enough computing power and memory capacity to run local AI models with up to 120 billion parameters on the NPU alone, without needing additional memory or computing resources.Dell is outfitting these laptops with robust internals, including up to an Intel Core Ultra 9 285HX vPro CPU with 24 cores. Notably, Dell has removed the discrete GPU to accommodate the NPU, which offers more computing power and VRAM capacity than a traditional GPU. For local AI development, the system utilizes this NPU for all necessary tasks, paired with a CPU and 64 GB of memory. For graphics output, the system relies on the integrated GPU within the ""Arrow Lake-HX"" chip, which handles basic graphics tasks but isn't suitable for gaming. This design choice is logical, as the dedicated NPU is intended for AI developers and occupies the space, power, and heat dissipation capacity that a GPU would typically use.Dell is developing laptops with dedicated NPUs to meet the demands of modern workloads that require fast, local AI processing without relying on the cloud. Tasks such as analyzing medical scans, detecting fraud in financial data, modeling complex engineering problems, and handling sensitive research demand immediate results and strict data privacy. These tasks cannot afford network latency or the risk of sending confidential information off the device. They benefit from an AI processor capable of handling large models running continuously. By incorporating powerful NPUs, Dell aims to serve professionals who need real-time insights, predictable performance, and complete control over their data, all from a portable machine that integrates seamlessly into their daily workflow."
3,Can Bezos’ Project Prometheus Close America’s Widening Physical-AI Gap with China?,https://techreport.com/news/bezos-project-prometheus-close-widening-us-china-physical-ai-gap/,Techreport.com,2025-11-21T17:03:11Z,"Key takeaways: Jeff Bezos becomes co-CEO in a new AI startup called Prometheus, aiming to build AI for the physical world.

Prometheus will focus on integrating artificial intelligence with sectors like manufacturing, robotics, heavy machinery, and everything industrial that works on ‘atoms’ and not ‘bits.’

Prometheus can be the US’ answer to China’s advanced industrial robot integrations and smart factories.

Jeff Bezos has taken the role of co-CEO in a new AI startup named Project Prometheus. What’s interesting is that this is Bezos’ first position in management since he stepped down as Amazon CEO in July 2021.

Project Prometheus describes itself as a company that focuses on ‘AI for the physical economy.’ Simply put, Prometheus will focus on the application of AI in industries that require advanced engineering and materials science.

This includes all manufacturing activities related to the production of physical goods, such as automobiles, electronics, heavy machinery, and aerospace manufacturing.

Up until now, the entire focus of artificial intelligence has been on ‘bits,’ which is why we have several Large Language Models (LLMs) today powering numerous chatbots and AI assistants.

However, Prometheus wants to focus on ‘AI for atoms,’ where artificial intelligence can be used to stimulate physics, automate factories, design physical systems, and reduce manufacturing times.

Prometheus debuts in an already overcrowded AI market where several small and big players are fighting to create a niche. However, one of Prometheus’ biggest advantages is the huge $6.2B funding it has secured, a large part of which comes from Bezos himself. This makes it one of the most well-funded early-stage startups globally.

In comparison, Periodic Labs, a company that aims to ‘automate scientific discovery’ by focusing on physical AI (much like Prometheus), raised $2B this year. So, of course, Prometheus’ chunky wallet gives it a head-start over its rivals.

China’s Lead in the Physical AI Race

Although the US has attempted to hinder China’s growth in the AI sector through strict export controls on advanced AI-compatible semiconductor chips, the fact remains that China is significantly ahead of the US in terms of physical AI applications.

Data from the International Federation of Robotics shows that China is third on the list of countries with the highest robot density. The country has 470 robots per 10,000 employees, which is up from 402 robots in 2022.

What’s even more commendable is that China entered the top 10 list only in 2019, and in just 6 years, it has raced past countries like Japan and Germany. On the other hand, the US ranks 10th on the list with a density of only 295 robots.

A high robot density points to a highly automated and productive manufacturing sector with relatively fewer defects and faster iteration cycles. Throw a bit of AI tech in there, and you have ‘smart factories’ with automated control systems, AI-backed quality inspections, predictive maintenance, and so on.

Along with this, China also seems to have a large talent pool to tap into:

China produces 1,059 engineering bachelor’s graduates per million people (2025 estimate) as compared to the US’s 403.

China graduates 1.5M+ bachelor’s engineers annually, along with 400K master’s and 60K PhDs. By contrast, the US awards just 140K bachelors with 50K master’s and 12K PhDs.

No wonder top US AI startups have to poach talent from other leading companies, something that Prometheus has also done.

China has already surpassed the US in manufacturing, to the extent that it is now known as the world’s factory. It has left the US behind in areas like advanced battery manufacturing, solar production, EV supply chains, and robotics deployment.

After being left behind in critical physical industries, the US can only play catch-up now, which is already a steep uphill climb. Deploying Physical AI might be the only way the US can get close to China. But that isn’t an easy task in itself.

For instance, Marques Brownlee, a popular tech reviewer, discussed NEO, the most futuristic robot available in the US today. Although the company markets it as a smart robot, as things stand now, it’s anything but smart and autonomous. It works largely on human command and only does a handful of tasks without instructions.

Brownlee noted that ‘AI companies’ are currently only selling the dream, before even fully creating the product. They’re using early-stage adopters as beta testers, but the gap between the blueprints and the final product is a huge one right now.

China is already deploying humanoid robots across frontline industrial applications. For instance, the Walker S2 from UBTECH, China’s most advanced humanoid robot, is capable of running 24/7 and can even replace its own battery.

Compare this with NEO’s 4-hour battery life and teleoperation needs, and you’ll realize that the gap is wider than most Americans would want to believe. UBTECH plans to hit an annual capacity of 10,000 Walker S2 by 2027 for industrial use in smart factories, intelligent logistics, and automotive manufacturing.

Bezos’ Mission Critical Role

If there’s one man fit to lead such a herculean effort, it’s Jeff Bezos. His portfolio and experience are something even the brightest modern-day entrepreneur envies.

Bezos built the world’s largest logistics network, the most advanced warehousing system, a rocket company in Blue Origin, a cloud computing behemoth in AWS, and a robotics powerhouse in Amazon Robotics. That’s a solid CV.

Prometheus wants to build advanced physical stimulators, industrial-grade AI systems, and engineering automation tools, something that Bezos has dabbled in for 30 years.

Most importantly, he brings in the huge capital required by such a research-intensive operation. In addition to spending money out of his own pocket, the Bezos branding also helps attract top investors in the country, makes it easier to recruit top talent, and reassures investors about the viability of the mission.

Vik Bajaj, the other co-CEO of Prometheus, is also a very well-thought-out selection for the job. At a time when most AI leaders come with a background in machine learning and natural language processing, Bajaj comes from an experience in bioinformatics, biophysics, and molecular simulation.

He was the co-founder of Google’s Life Sciences division (which we now know as Verily), which is all about navigating huge R&D gestation periods and integrating robotics, sensors, AI, and cloud systems.

While Bezos is the face of the project, Vik Bajaj brings the hands-on experience and technological know-how required to tackle a business as large as this.

Unlike LLM-based AI startups, results for the physical AI space don’t come within just six months or a year. The R&D and trial-and-error phase could last for nearly half a decade, if not more.

A business this capital-intensive with a long gestation like this requires the support of someone who’s been there before – and both Bezos and Bajaj fit the bill perfectly.

Krishi is a seasoned tech journalist with over four years of experience writing about PC hardware, consumer technology, and artificial intelligence. Clarity and accessibility are at the core of Krishi’s writing style. He believes technology writing should empower readers—not confuse them—and he’s committed to ensuring his content is always easy to understand without sacrificing accuracy or depth. Over the years, Krishi has contributed to some of the most reputable names in the industry, including Techopedia, TechRadar, and Tom’s Guide. A man of many talents, Krishi has also proven his mettle as a crypto writer, tackling complex topics with both ease and zeal. His work spans various formats—from in-depth explainers and news coverage to feature pieces and buying guides. Behind the scenes, Krishi operates from a dual-monitor setup (including a 29-inch LG UltraWide) that’s always buzzing with news feeds, technical documentation, and research notes, as well as the occasional gaming sessions that keep him fresh. Krishi thrives on staying current, always ready to dive into the latest announcements, industry shifts, and their far-reaching impacts. When he's not deep into research on the latest PC hardware news, Krishi would love to chat with you about day trading and the financial markets—oh! And cricket, as well. View all articles by Krishi Chowdhary

Related Articles"
4,CrowdStrike fires ‘suspicious insider’ who passed information to hackers,https://biztoc.com/x/b33d3e61c4dfd515,Biztoc.com,2025-11-21T19:05:14Z,"Cybersecurity giant CrowdStrike denied it had been hacked following claims from a hacker group, which leaked screenshots from inside CrowdStrike's network.

This story appeared on techcrunch.com , 2025-11-21 18:55:53."
5,CrowdStrike fires 'suspicious insider' who passed information to hackers | TechCrunch,https://techcrunch.com/2025/11/21/crowdstrike-fires-suspicious-insider-who-passed-information-to-hackers/,TechCrunch,2025-11-21T18:58:39Z,"Cybersecurity giant CrowdStrike has confirmed firing a “suspicious insider” last month who allegedly fed information about the company to a notorious hacking group.

A hacking collective known as Scattered Lapsus$ Hunters published screenshots late Thursday and Friday morning in a public Telegram channel that allegedly showed insider access to CrowdStrike systems. The screenshots, which TechCrunch has seen, show dashboards containing links to company resources, including a user’s Okta dashboard used by employees for accessing internal apps.

The hackers claimed in the Telegram channel to have compromised CrowdStrike through a recent breach at Gainsight, a customer relationship management company that helps Salesforce customers track and manage their own customers’ data. The hackers said they used information stolen from Gainsight to break into CrowdStrike.

But CrowdStrike says the hackers’ claims are “false,” and says it terminated the insider’s access after the company “determined he shared pictures of his computer screen externally.”

“Our systems were never compromised and customers remained protected throughout. We have turned the case over to relevant law enforcement agencies,” CrowdStrike spokesperson Kevin Benacci told TechCrunch.

Several other tech companies were allegedly hacked as part of the same campaign. Gainsight did not respond to TechCrunch’s requests for comment.

Scattered Lapsus$ Hunters is a collective of hackers made up of several hacking groups, notably ShinyHunters, Scattered Spider, and Lapsus$. The group’s members use social engineering techniques to trick employees into granting them access to their systems or databases.

In October, Scattered Lapsus$ Hunters claimed to have stolen more than 1 billion records from corporate giants who rely on Salesforce to host their customer data. The hackers published a data leak site listing data stolen from companies, including insurance giant Allianz Life, the airline Qantas, carmaker Stellantis, credit bureau TransUnion, the employee management platform Workday, and others."
6,AI teddy bear for kids responds with sexual content and advice about weapons,https://www.malwarebytes.com/blog/news/2025/11/ai-teddy-bear-for-kids-responds-with-sexual-content-and-advice-about-weapons,Malwarebytes.com,2025-11-21T18:45:32Z,"In testing, FoloToy’s AI teddy bear jumped from friendly chat to sexual topics and unsafe household advice. It shows how easily artificial intelligence can cross serious boundaries. It’s a fair moment to ask whether AI-powered stuffed animals are appropriate for children.

It’s easy to get swept up in the excitement of artificial intelligence, especially when it’s packaged as a plush teddy bear promising

“warmth, fun, and a little extra curiosity.”

But the recent controversy surrounding the Kumma bear is a reminder to slow down and ask harder questions about putting AI into toys for kids.

FoloToy, a Singapore-based toy company, marketed the $99 bear as the ultimate “friend for both kids and adults,” leveraging powerful conversational AI to deliver interactive stories and playful banter. The website described Kumma as intelligent and safe. Behind the scenes, the bear used OpenAI’s language model to generate its conversational responses. Unfortunately, reality didn’t match the sales pitch.

Image courtesy of CNN, a screenshot taken from FoloToy’s website

According to a report from the US PIRG Education Fund, Kumma quickly veered into wildly inappropriate territory during researcher tests. Conversations escalated from innocent to sexual within minutes. The bear didn’t just respond to explicit prompts, which would have been more or less understandable. Researchers said it introduced graphic sexual concepts on its own, including BDSM-related topics, explained “knots for beginners,” and referenced roleplay scenarios involving children and adults. In some conversations, Kumma also probed for personal details or offered advice involving dangerous objects in the home.

It’s unclear whether the toy’s supposed safeguards against inappropriate content were missing or simply didn’t work. While children are unlikely to introduce BDSM as a topic to their teddy bear, the researchers warned just how low the bar was for Kumma to cross serious boundaries.

The fallout was swift. FoloToy suspended sales of Kumma and other AI-enabled toys, while OpenAI revoked the developer’s access for policy violations. But as PIRG researchers note, that response was reactive. Plenty of AI toys remain unregulated, and the risks aren’t limited to one product.

Which proves our point: AI does not automatically make something better. When companies rush out “smart” features without real safety checks, the risks fall on the people using them—especially children, who can’t recognize dangerous content when they see it.

Tips for staying safe with AI toys and gadgets

You’ll see “AI-powered” on almost everything right now, but there are ways to make safer choices.

Always research: Check for third-party safety reviews before buying any AI-enabled product marketed for kids.

Check for third-party safety reviews before buying any AI-enabled product marketed for kids. Test first, supervise always: Interact with the device yourself before giving it to children. Monitor usage for odd or risky responses.

Interact with the device yourself before giving it to children. Monitor usage for odd or risky responses. Use parental controls: If available, enable all content filters and privacy protections.

If available, enable all content filters and privacy protections. Report problems: If devices show inappropriate content, report to manufacturers and consumer protection groups.

If devices show inappropriate content, report to manufacturers and consumer protection groups. Check communications: Find out what the device collects, who it shares data with, and what it uses the information for.

But above all, remember that not all “smart” is safe. Sometimes, plush, simple, and old-fashioned really is better.

AI may be everywhere, but designers and buyers alike need to put safety, privacy, and common sense ahead of the technological wow-factor.

We don’t just report on data privacy—we help you remove your personal information

Cybersecurity risks should never spread beyond a headline. With Malwarebytes Personal Data Remover, you can scan to find out which sites are exposing your personal information, and then delete that sensitive data from the internet."
7,CrowdStrike confirms that an insider shared screenshots from internal systems with unnamed threat actors but says its systems were not breached (Sergiu Gatlan/BleepingComputer),https://www.techmeme.com/251121/p22,Techmeme.com,2025-11-21T18:30:01Z,— This week the company announced that it would require users to undergo an A.I.-powered age estimation process in order to chat with others on the platform.
8,"Wojak ($WOJAK) Launches on Solana, Bringing Internet’s Most Recognized Meme to a High-Performance Blockchain",https://www.globenewswire.com/news-release/2025/11/21/3192954/0/en/Wojak-WOJAK-Launches-on-Solana-Bringing-Internet-s-Most-Recognized-Meme-to-a-High-Performance-Blockchain.html,GlobeNewswire,2025-11-21T18:11:00Z,"TORONTO, Nov. 21, 2025 (GLOBE NEWSWIRE) -- Wojak ($WOJAK), one of the most iconic and widely recognized memes in internet culture, has officially launched on the Solana blockchain. Known globally as the “Feels Guy,” Wojak is a foundational cultural figure whose broad emotional expression has defined online communities for over a decade. The new $WOJAK token introduces this enduring cultural symbol to the rapidly expanding, high-throughput Solana ecosystem.





A Cultural Icon Enters Modern Infrastructure

Wojak is an MS Paint-style illustration used to convey a wide spectrum of human emotions—from melancholy and reflection to triumph, nostalgia, and shared community sentiment. Its versatility has led to significant evolution; a search for “Wojak” online produces more than 200 catalogued variations, transforming the character into a truly universal language for the internet.

With the launch of $WOJAK on Solana, the character moves from a cultural icon into a blockchain asset, entering an environment known for its high throughput, near-instant confirmations, and low-cost transactions. This technical combination positions the project for rapid community growth and scalable adoption.

The Advantage of Solana

Solana has established itself as one of the leading ecosystems for community and meme-driven projects, primarily due to its infrastructure's ability to handle high transaction volumes and micro-transactions efficiently. For a widely referenced meme like Wojak, Solana’s foundation provides the ideal environment for viral expansion and trading activity, with significantly reduced friction compared to slower or more expensive networks.

For years, Wojak has been embedded in forums, social media, and crypto communities, frequently reflecting market sentiment in a way few other memes can. The image has been adapted into numerous variations—including Doomer Wojak, Zoomer Wojak, NPC Wojak, and Based Wojak—each representing its own subculture and emotional context.

Project Objectives

The introduction of $WOJAK on Solana achieves two key objectives for this digital expression:

Preservation: Placing a foundational internet meme on an immutable ledger. Community Building: Allowing global communities to build new utilities, artworks, and tokenized experiences around a universally understood character.

The $WOJAK initiative aims to combine the cultural longevity of the meme with Solana’s growing influence in the market. The project's vision centers on accessibility, decentralization, and community involvement, leveraging the familiarity of the Wojak symbol to create a new digital asset that resonates with both long-time internet users and new participants in the Solana environment.

About Wojak ($WOJAK)

Wojak ($WOJAK) is a Solana-based token inspired by one of the internet’s most enduring and versatile memes. Built for cultural relevance and long-term community engagement, $WOJAK aims to bring the broad emotional language of the Wojak character into the blockchain space. The project emphasizes open participation, decentralized ownership, and seamless user experience enabled by Solana’s high-performance architecture.

Twitter (X): https://x.com/wojakonx

Telegram: https://t.me/Wojak8J69

Website: https://wojakmeme.org/

CMC: https://coinmarketcap.com/currencies/wojak-sol/

CoinGecko: https://www.coingecko.com/en/coins/wojak-4

MEDIA CONTACT

Contact Person: Alex Blinko

Company: Wojak

Email: info@wojak.meme

Website: https://wojakmeme.org/

Disclaimer: This content is provided by Wojak. The statements, views, and opinions expressed in this content are solely those of the content provider and do not necessarily reflect the views of this media platform or its publisher. We do not endorse, verify, or guarantee the accuracy, completeness, or reliability of any information presented. We do not guarantee any claims, statements, or promises made in this article. This content is for informational purposes only and should not be considered financial, investment, or trading advice. Investing in crypto and mining-related opportunities involves significant risks, including the potential loss of capital. It is possible to lose all your capital. These products may not be suitable for everyone, and you should ensure that you understand the risks involved. Seek independent advice if necessary. Speculate only with funds that you can afford to lose. Readers are strongly encouraged to conduct their own research and consult with a qualified financial advisor before making any investment decisions. However, due to the inherently speculative nature of the blockchain sector—including cryptocurrency, NFTs, and mining—complete accuracy cannot always be guaranteed. Neither the media platform nor the publisher shall be held responsible for any fraudulent activities, misrepresentations, or financial losses arising from the content of this press release. In the event of any legal claims or charges against this article, we accept no liability or responsibility. Globenewswire does not endorse any content on this page.

Legal Disclaimer: This media platform provides the content of this article on an ""as-is"" basis, without any warranties or representations of any kind, express or implied. We assume no responsibility for any inaccuracies, errors, or omissions. We do not assume any responsibility or liability for the accuracy, content, images, videos, licenses, completeness, legality, or reliability of the information presented herein. Any concerns, complaints, or copyright issues related to this article should be directed to the content provider mentioned above.

A photo accompanying this announcement is available at https://www.globenewswire.com/NewsRoom/AttachmentNg/7f29a0b6-c407-4d76-bcec-66cad58a4a92"
9,"Build AI Agents Using Low-Code Tools Like LangGraph, CrewAI, Zapier, and Bubble Course Launched - Design Autonomous Multi-Agent Systems",https://www.globenewswire.com/news-release/2025/11/21/3192939/0/en/Build-AI-Agents-Using-Low-Code-Tools-Like-LangGraph-CrewAI-Zapier-and-Bubble-Course-Launched-Design-Autonomous-Multi-Agent-Systems.html,GlobeNewswire,2025-11-21T17:42:00Z,"SANTA CLARA, CA, Nov. 21, 2025 (GLOBE NEWSWIRE) -- Interview Kickstart, a U.S.-based upskilling platform for technology professionals, has announced the launch of its Applied Agentic AI course, a 12-week program designed to help participants develop practical skills in designing and deploying autonomous AI agents. As organizations incorporate AI-driven automation to streamline operations and enhance decision-making, professionals with applied experience in agentic systems are becoming increasingly important in both engineering and enterprise environments. To learn more about ""How To Build AI Agents With Low Code Tools Like LangGraph, CrewAI, Zapier, and Bubble"" course visit the Interview Kickstart website.

Agentic AI refers to systems that move beyond predictive modeling to execute actions autonomously within defined workflows. These systems can perform tasks such as data retrieval, process automation, scheduling, content generation, or system-to-system communication without manual intervention. As companies adopt these capabilities, the need for engineers and analysts who can build, configure, and maintain agent-based systems continues to grow.

Interview Kickstart's Applied Agentic AI course was developed to address this emerging skills gap. The curriculum combines conceptual instruction with hands-on learning to help participants build functioning agentic workflows using widely adopted tools in the low-code and no-code ecosystem. The program includes training on platforms such as LangGraph, CrewAI, Zapier, and Bubble, enabling learners to design and test agent workflows without requiring deep software engineering backgrounds.

Participants attend live sessions led by AI practitioners from FAANG and other major technology organizations. These instructors offer insight into practical agent deployment, system integration, and the operational considerations involved in building autonomous AI systems. Their experience provides learners with exposure to the technical and strategic requirements of agent-based automation in production environments.

The course covers several core topics, including prompt engineering, multi-agent system design, function calling, API integration, and workflow automation. Instruction emphasizes applied skills that mirror real-world implementation patterns, helping learners understand how agentic systems interact with databases, third-party APIs, enterprise software tools, and internal operational pipelines.

A key component of the Applied Agentic AI program is its four live guided projects, which simulate operational use cases. Throughout these projects, participants build and test end-to-end agent workflows that reflect industry applications such as customer support routing, marketing operations automation, product feature prototyping, and internal process optimization. These project-based experiences help learners develop practical capabilities that can be transferred to workplace environments.

""Agentic AI is becoming central to how modern systems operate,"" said a spokesperson for Interview Kickstart. ""This program helps professionals understand not only the conceptual foundation of autonomous agent systems but also how to apply these concepts to real tasks, workflows, and product requirements.""

The course is designed for working professionals seeking to expand their capabilities in automation, AI product development, and intelligent system design. Engineers, data scientists, product managers, and operations professionals can apply the curriculum to a range of use cases, from prototyping internal tools to implementing automation within enterprise systems.

With rising interest in AI workflow automation across industries, Interview Kickstart's Applied Agentic AI course reflects the organization's continued focus on industry-aligned training. The 12-week structure encourages consistent engagement through live instruction, collaborative discussions, and guided project work, providing a balance of flexibility and rigor.

The launch of the Applied Agentic AI course expands Interview Kickstart's suite of AI- and engineering-focused programs. As organizations develop more autonomous and intelligent systems, professionals who understand both AI theory and applied agentic design will be positioned to contribute to emerging automation initiatives. More information about the program is available at: https://interviewkickstart.com/courses/agentic-ai-for-tech-low-code

https://youtu.be/VL9wREmvxMw?si=6BcpfB3RIcBrKgK0

About Interview Kickstart

Founded in 2014, Interview Kickstart is an upskilling platform that provides training programs for software engineers, data professionals, product managers, and other technology practitioners. With more than 20,000 alumni, the platform delivers structured learning through live instruction, recorded content, mock interviews, and personalized mentorship. Its network of 700+ instructors, including hiring managers and technical leaders from major technology companies, contributes to the development of its curriculum and instructional methods.

###

For more information about Interview Kickstart, contact the company here:



Interview Kickstart

Burhanuddin Pithawala

+1 (209) 899-1463

aiml@interviewkickstart.com

4701 Patrick Henry Dr Bldg 25, Santa Clara, CA 95054, United States"
